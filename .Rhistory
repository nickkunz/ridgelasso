alpha = 1, # 0 = ridge, 1 = lasso
lambda = lambda)
## ridge regression dimension
dim(coef(ridge))
## lasso regression dimension
dim(coef(lasso))
## specific lambda
round(ridge$lambda[50])
## ridge regression coefficients
coef(ridge)[,50]
## specific lambda
round(ridge$lambda[60])
## ridge regression coefficients
coef(ridge)[,60]
## ridge regression prediction function
predict(ridge,
s = 50,  # lambda
type = "coefficients")[1:20,]
## random number
set.seed(1)
## training and test split
train = sample(1:nrow(x), nrow(x)/2)  # create training data
test = (-train)  # create test data
y.test = y[test]  # create test
## ridge regression training
ridge_model = glmnet(x[train,],
y[train],
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
plot(ridge_model, xvar = "lambda")
## ridge regression prediction, λ = 75
ridge_predict_75 = predict(ridge_model,
s = 75,  # lambda
newx = x[test,])
## test MSE, λ = 75
round(mean((ridge_predict_75 - y.test)^2))
## ridge regression prediction, λ = 4
ridge_predict_4 = predict(ridge_model,
s = 4,  # lambda
newx = x[test,])
## test MSE, λ = 4
round(mean((ridge_predict_4 - y.test)^2))
## random number
set.seed(1)
## ridge regression training & cross-validation
cv.ridge_out = cv.glmnet(x[train ,],
y[train],
alpha = 0)  # 0 = ridge, 1 = lasso
## ridge regression, λ = optimal
optimal_lambda = cv.ridge_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.ridge_out)
## ridge regression prediction, λ = optimal
ridge_predict_optimal = predict(ridge_model,
s = optimal_lambda,  # lambda
newx = x[test,])
## Test MSE, λ = optimal
round(mean((ridge_predict_optimal - y.test)^2))
## refit ridge regression
ridge_out = glmnet(x = x,
y = y,
alpha = 0)  # 0 = ridge, 1 = lasso
## refit ridge regression coefficients
predict(ridge_out,
s = optimal_lambda,  # lambda
type = "coefficients")[1:20,]
## visualize refit ridge regression coefficients
plot(ridge_out, xvar = "lambda")
## lasso regression training
lasso_model = glmnet(x[train ,],
y[train],
alpha = 1,  # 0 = ridge, 1 = lasso
lambda = lambda)
plot(lasso_model, xvar = "lambda")
## random number
set.seed (1)
## lasso regression training & cross-validation
cv.lasso_out = cv.glmnet(x[train ,],
y[train],
alpha = 1)  # 0 = ridge, 1 = lasso
## lasso regression, λ = optimal
optimal_lambda = cv.lasso_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.lasso_out)
## lasso regression prediction, λ = optimal
lasso_predict_optimal = predict(lasso_model,
s = optimal_lambda,
newx = x[test,])
## lasso test MSE, λ = optimal
round(mean((lasso_predict_optimal - y.test)^2))
## refit lasso regression
lasso_out = glmnet(x = x,
y = y,
alpha = 1)  # 0 = ridge, 1 = lasso
## refit lasso coefficients
predict(lasso_out,
s = optimal_lambda,
type = "coefficients")[1:20,]
## visualize refit lasso coefficients
plot(lasso_out, xvar = "lambda")
plot(lasso_model)
## visualize refit lasso coefficients
plot(lasso_out)
## visualize refit lasso coefficients
plot(lasso_outout)
## refit lasso coefficients
lasso_outout = predict(lasso_out,
s = optimal_lambda,
type = "coefficients")[1:20,]
## visualize refit lasso coefficients
plot(lasso_outout)
## refit lasso coefficients
predict(lasso_out,
s = optimal_lambda,
type = "coefficients")[1:20,]
plot(lasso_model, xvar = "lambda")
)
)
## lasso regression training
lasso_model = glmnet(x[train ,],
y[train],
alpha = 1,  # 0 = ridge, 1 = lasso
lambda = lambda)
plot(lasso_model)
## visualize refit ridge regression coefficients
plot(ridge_out)
plot(ridge_model)
plot(ridge_model, xvar = "lambda")
## visualize refit ridge regression coefficients
plot(ridge_out, xvar = "lambda")
plot(lasso_model)
plot(lasso_model, xvar = "lambda")
## lasso regression training
lasso_model = glmnet(x[train ,],
y[train],
alpha = 1,  # 0 = ridge, 1 = lasso
lambda = lambda)
plot(lasso_model, xvar = "lambda")
## visualize refit lasso coefficients
plot(lasso_out, xvar = "lambda")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "lambda")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "norm")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "lambda")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "norm")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "lambda")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "norm")
## visualize refit lasso coefficients
plot(lasso_model, xvar = "dev")
## visualize lasso coefficients by fraction of deviance
plot(lasso_model, xvar = "dev")
## visualize lasso coefficients by lambda
plot(lasso_model, xvar = "lambda")
## visualize lasso coefficients by l1 norm
plot(lasso_model, xvar = "norm")
## visualize lasso coefficients by fraction of deviance
plot(lasso_model, xvar = "dev")
lasso_model
predict(lasso_model,
s = lambda,
type = "coefficients")[1:20,]
knitr::opts_chunk$set(echo = TRUE)
## load libraries
library(ISLR)  # data
library(glmnet)  # regression
## load data
Hitters = ISLR::Hitters
## remove all observations missing data
Hitters = na.omit(Hitters)
## view feature names
names(Hitters)
## view data frame dimension
dim(Hitters)
## view first data frame observations
head(Hitters)
## operationalize data
x = model.matrix(Salary~., Hitters)[,-1]  # create dummy independent variable features
y = Hitters$Salary  # create dependent variable feature
## specify lambda
lambda = 10 ^ seq(from = 10,
to = -2,
length = 100)
## ridge regression
ridge = glmnet(x = x,
y = y,
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## lasso regression
lasso = glmnet(x = x,
y = y,
alpha = 1, # 0 = ridge, 1 = lasso
lambda = lambda)
## ridge regression dimension
dim(coef(ridge))
## lasso regression dimension
dim(coef(lasso))
## specific lambda
round(ridge$lambda[50])
## ridge regression coefficients
coef(ridge)[,50]
## specific lambda
round(ridge$lambda[60])
## ridge regression coefficients
coef(ridge)[,60]
## ridge regression prediction function
predict(ridge,
s = 50,  # lambda
type = "coefficients")[1:20,]
## random number
set.seed(1)
## training and test split
train = sample(1:nrow(x), nrow(x)/2)  # create training data
test = (-train)  # create test data
y.test = y[test]  # create test
## ridge regression training
ridge_model = glmnet(x[train,],
y[train],
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## visualize lasso regression coefficients by lambda
plot(lasso_model, xvar = "lambda")
knitr::opts_chunk$set(echo = TRUE)
## load libraries
library(ISLR)  # data
library(glmnet)  # regression
## load data
Hitters = ISLR::Hitters
## remove all observations missing data
Hitters = na.omit(Hitters)
## view feature names
names(Hitters)
## view data frame dimension
dim(Hitters)
## view first data frame observations
head(Hitters)
## operationalize data
x = model.matrix(Salary~., Hitters)[,-1]  # create dummy independent variable features
y = Hitters$Salary  # create dependent variable feature
## specify lambda
lambda = 10 ^ seq(from = 10,
to = -2,
length = 100)
## ridge regression
ridge = glmnet(x = x,
y = y,
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## lasso regression
lasso = glmnet(x = x,
y = y,
alpha = 1, # 0 = ridge, 1 = lasso
lambda = lambda)
## ridge regression dimension
dim(coef(ridge))
## lasso regression dimension
dim(coef(lasso))
## specific lambda
round(ridge$lambda[50])
## ridge regression coefficients
coef(ridge)[,50]
## specific lambda
round(ridge$lambda[60])
## ridge regression coefficients
coef(ridge)[,60]
## ridge regression prediction function
predict(ridge,
s = 50,  # lambda
type = "coefficients")[1:20,]
## random number
set.seed(1)
## training and test split
train = sample(1:nrow(x), nrow(x)/2)  # create training data
test = (-train)  # create test data
y.test = y[test]  # create test
## ridge regression training
ridge_model = glmnet(x[train,],
y[train],
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## visualize lasso regression coefficients by lambda
plot(ridge_model, xvar = "lambda")
## visualize lasso regression coefficients by l1 norm
plot(ridge_model, xvar = "norm")
## visualize lasso regression coefficients by fraction of deviance
plot(ridge_model, xvar = "dev")
## ridge regression prediction, λ = 75
ridge_predict_75 = predict(ridge_model,
s = 75,  # lambda
newx = x[test,])
## test MSE, λ = 75
round(mean((ridge_predict_75 - y.test)^2))
## ridge regression prediction, λ = 4
ridge_predict_4 = predict(ridge_model,
s = 4,  # lambda
newx = x[test,])
## test MSE, λ = 4
round(mean((ridge_predict_4 - y.test)^2))
## random number
set.seed(1)
## ridge regression training & cross-validation
cv.ridge_out = cv.glmnet(x[train ,],
y[train],
alpha = 0)  # 0 = ridge, 1 = lasso
## ridge regression, λ = optimal
optimal_lambda = cv.ridge_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.ridge_out)
## ridge regression prediction, λ = optimal
ridge_predict_optimal = predict(ridge_model,
s = optimal_lambda,  # lambda
newx = x[test,])
## Test MSE, λ = optimal
round(mean((ridge_predict_optimal - y.test)^2))
## refit ridge regression
ridge_out = glmnet(x = x,
y = y,
alpha = 0)  # 0 = ridge, 1 = lasso
## refit ridge regression coefficients
predict(ridge_out,
s = optimal_lambda,  # lambda
type = "coefficients")[1:20,]
## lasso regression training
lasso_model = glmnet(x[train ,],
y[train],
alpha = 1,  # 0 = ridge, 1 = lasso
lambda = lambda)
## visualize lasso regression coefficients by lambda
plot(lasso_model, xvar = "lambda")
## visualize lasso regression coefficients by l1 norm
plot(lasso_model, xvar = "norm")
## visualize lasso regression coefficients by fraction of deviance
plot(lasso_model, xvar = "dev")
## random number
set.seed (1)
## lasso regression training & cross-validation
cv.lasso_out = cv.glmnet(x[train ,],
y[train],
alpha = 1)  # 0 = ridge, 1 = lasso
## lasso regression, λ = optimal
optimal_lambda = cv.lasso_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.lasso_out)
## lasso regression prediction, λ = optimal
lasso_predict_optimal = predict(lasso_model,
s = optimal_lambda,
newx = x[test,])
## lasso test MSE, λ = optimal
round(mean((lasso_predict_optimal - y.test)^2))
## refit lasso regression
lasso_out = glmnet(x = x,
y = y,
alpha = 1)  # 0 = ridge, 1 = lasso
## refit lasso coefficients
predict(lasso_out,
s = optimal_lambda,
type = "coefficients")[1:20,]
source('~/github/ridgelasso.R', echo=TRUE)
## data inspection
head(Hitters)  ## preview observations
source('~/github/ridgelasso.R', echo=TRUE)
coef(ridge_model)
coef(ridge_model)[Salary]
## ridge regression coefficients
coef(ridge)[,50]
source('~/github/ridgelasso.R', echo=TRUE)
source('~/github/ridgelasso.R', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
## load libraries
library(ISLR)  # data
library(glmnet)  # regression
## load data
Hitters = ISLR::Hitters
## remove all observations missing data
Hitters = na.omit(Hitters)
## view feature names
names(Hitters)
## view data frame dimension
dim(Hitters)
## view first data frame observations
head(Hitters)
## operationalize data
x = model.matrix(Salary~., Hitters)[,-1]  # create dummy independent variable features
y = Hitters$Salary  # create dependent variable feature
## specify lambda
lambda = 10 ^ seq(from = 10,
to = -2,
length = 100)
## ridge regression
ridge = glmnet(x = x,
y = y,
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## lasso regression
lasso = glmnet(x = x,
y = y,
alpha = 1, # 0 = ridge, 1 = lasso
lambda = lambda)
## ridge regression dimension
dim(coef(ridge))
## lasso regression dimension
dim(coef(lasso))
## specific lambda
round(ridge$lambda[50])
## ridge regression coefficients
coef(ridge)[,50]
## specific lambda
round(ridge$lambda[60])
## ridge regression coefficients
coef(ridge)[,60]
## ridge regression prediction function
predict(ridge,
s = 50,  # lambda
type = "coefficients")[1:20,]
## random number
set.seed(1)
## training and test split
train = sample(1:nrow(x), nrow(x)/2)  # create training data
test = (-train)  # create test data
y.test = y[test]  # create test
## ridge regression training
ridge_model = glmnet(x[train,],
y[train],
alpha = 0,  # 0 = ridge, 1 = lasso
lambda = lambda)
## visualize ridge regression coefficients by log-lambda
plot(ridge_model, xvar = "lambda")
## visualize ridge regression coefficients by l1 norm
plot(ridge_model, xvar = "norm")
## visualize ridge regression coefficients by fraction of deviance
plot(ridge_model, xvar = "dev")
## ridge regression prediction, λ = 75
ridge_predict_75 = predict(ridge_model,
s = 75,  # lambda
newx = x[test,])
## test MSE, λ = 75
round(mean((ridge_predict_75 - y.test)^2))
## ridge regression prediction, λ = 4
ridge_predict_4 = predict(ridge_model,
s = 4,  # lambda
newx = x[test,])
## test MSE, λ = 4
round(mean((ridge_predict_4 - y.test)^2))
## random number
set.seed(1)
## ridge regression training & cross-validation
cv.ridge_out = cv.glmnet(x[train ,],
y[train],
alpha = 0)  # 0 = ridge, 1 = lasso
## ridge regression, λ = optimal
optimal_lambda = cv.ridge_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.ridge_out)
## ridge regression prediction, λ = optimal
ridge_predict_optimal = predict(ridge_model,
s = optimal_lambda,  # lambda
newx = x[test,])
## test MSE, λ = optimal
round(mean((ridge_predict_optimal - y.test)^2))
## refit ridge regression
ridge_out = glmnet(x = x,
y = y,
alpha = 0)  # 0 = ridge, 1 = lasso
## refit ridge regression coefficients
predict(ridge_out,
s = optimal_lambda,  # lambda
type = "coefficients")[1:20,]
## lasso regression training
lasso_model = glmnet(x[train ,],
y[train],
alpha = 1,  # 0 = ridge, 1 = lasso
lambda = lambda)
## visualize lasso regression coefficients by log-lambda
plot(lasso_model, xvar = "lambda")
## visualize lasso regression coefficients by l1 norm
plot(lasso_model, xvar = "norm")
## visualize lasso regression coefficients by fraction of deviance
plot(lasso_model, xvar = "dev")
## random number
set.seed (1)
## lasso regression training & cross-validation
cv.lasso_out = cv.glmnet(x[train ,],
y[train],
alpha = 1)  # 0 = ridge, 1 = lasso
## lasso regression, λ = optimal
optimal_lambda = cv.lasso_out$lambda.min
round(optimal_lambda)
## plot ridge results, λ = optimal
plot(cv.lasso_out)
## lasso regression prediction, λ = optimal
lasso_predict_optimal = predict(lasso_model,
s = optimal_lambda,
newx = x[test,])
## lasso test MSE, λ = optimal
round(mean((lasso_predict_optimal - y.test)^2))
## refit lasso regression
lasso_out = glmnet(x = x,
y = y,
alpha = 1)  # 0 = ridge, 1 = lasso
## refit lasso coefficients
predict(lasso_out,
s = optimal_lambda,
type = "coefficients")[1:20,]
